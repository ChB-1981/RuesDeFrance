# -*- coding: utf-8 -*-
"""RuesDeFrance_CodeSource.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xpsCDhKzAWmYO56TisYvEUv6kh0m1xN3

#Projet #RuesDeFrance : code source**

---

Objectifs du projet : http://charleshenriboisseau.fr/2022/09/21/ruesdefrance-un-projet-data/

Rien n’est plus commun qu’une rue. Qu’est ce que leurs noms disent de nous ? Le projet #RuesDeFrance est une compilation des noms de 786000 rues françaises avec un zoom sur leurs occurrences et sur les personnalités. Ces lignes de code sont une préparation de la données avant analyse. Elles ont deux objectifs :  créer une liste propre de toutes les rues de France ; enrichir les rues aux noms de personnalités avec des données (genre, occupation etc.) sur ces personnalités.

Sources
*   https://www.lesruesdefrance.com
*   https://www.wikidata.org/
"""

import pandas as pd
import numpy as np
import requests
import shutil
import os
import time
import re
from bs4 import BeautifulSoup

"""# 1. Récupérer une liste propre de toutes les rues de France

**1.1- Importer les données à partir des url sources**
"""

"""
Opération : récupérer les données sur les url sources
Source : https://www.lesruesdefrance.com
Output : 101 fichiers csv pour plus de 7 millions d'entrées cumulées
"""


#Création d'une liste avec l'url des fichiers à télécharger
liste1 = list(range(1,19+1))
liste2 = list(range(21,29+1))
liste2.append('2A')
liste2.append('2B')
liste3 = list(range(30,95+1))
liste4 = list(range(971,974+1))
liste = liste1 + liste2 + liste3 + liste4
liste.append(976)
liste[0]='01'
liste[1]='02'
liste[2]='03'
liste[3]='04'
liste[4]='05'
liste[5]='06'
liste[6]='07'
liste[7]='08'
liste[8]='09'

liste_url = []
i = 0
while i < len(liste):
      url = f"https://www.lesruesdefrance.com/exportsql/liste_rue_par_dep_{liste[i]}.csv"
      response = requests.get(url)
      liste_url.append(url)
      i = i + 1

#Import des fichiers dans Google Colab
i = 0
while i < len(liste_url):
    r = requests.get(liste_url[i], stream=True)
    with open("fichier%s.csv" % i, "wb") as f:
        r.raw.decode_content = True
        shutil.copyfileobj(r.raw, f)
    i += 1
    time.sleep(0.5)

"""**2- Nettoyer et filtrer les données de chaque fichier csv**"""

"""
Opération : nettoyage et filtrage des fichiers un à un
Output : 101 fichiers csv propres, filtrés sur les rues, pour 786 000 entrées cumulées
"""

#Création de fichiers uniformisés centrés sur le rues
i = 0
while i <= 100:
  df_fichier = pd.read_csv('/content/fichier%s.csv' % i, sep=';')
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'L\'', 'L ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'D\'', 'D ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'S\'', 'S ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'T\'', 'T ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'  ', ' ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' -', ' ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' TRENTE ', ' 30 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' TRENTE ET UN ', ' 31 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT ET UN ', ' 21 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT DEUX ', ' 22 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT TROIS ', ' 23 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT QUATRE ', ' 24 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT CINQ ', ' 25 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT SIX ', ' 26 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT SEPT ', ' 27 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT HUIT ', ' 28 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT NEUF ', ' 29 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT ', ' 20 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' ONZE ', ' 11 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DOUZE ', ' 12 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' TREIZE ', ' 13 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' QUATORZE ', ' 14 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' QUINZE ', ' 15 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' SEIZE ', ' 16 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DIX SEPT ', ' 17 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DIX HUIT ', ' 18 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DIX NEUF ', ' 19 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT ', ' 20 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' UN ', ' 1 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DEUX ', ' 2 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' TROIS ', ' 3 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' QUATRE ', ' 4 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' CINQ ', ' 5 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' SIX ', ' 6 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' SEPT ', ' 7 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' HUIT ', ' 8 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' NEUF ', ' 9 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DIX ', ' 10 ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' GEN ', ' GENERAL ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' GAL ', ' GENERAL ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' MAL ', ' MARECHAL ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' COL ', ' COLONEL ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' CDT ', ' COMMANDANT ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DOC ', ' DOCTEUR ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' GDE ', ' GRANDE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'GEN ', 'GENERAL ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'GAL ', 'GENERAL ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'MAL ', 'MARECHAL ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'COL ', 'COLONEL ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'CDT ', 'COMMANDANT ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'DOC ', 'DOCTEUR ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'GDE ', 'GRANDE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'J J ', 'JEAN JACQUES ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'J P ', 'JEAN PIERRE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'J C ', 'JEAN CLAUDE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'J M ', 'JEAN MARIE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' CHEM ', ' CHEMIN ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'I ET F ', 'IRENE ET FREDERIC ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'P ET M ', 'PIERRE ET MARIE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ST ', 'SAINT ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'STE ', 'SAINTE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ASAINT ', 'AST ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ESAINT ', 'EST ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ISAINT ', 'IST ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'OSAINT ', 'OST ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'USAINT ', 'UST ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'YSAINT ', 'YST ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ASAINTE ', 'ASTE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ESAINTE ', 'ESTE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ISAINTE ', 'ISTE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'OSAINTE ', 'OSTE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'USAINTE ', 'USTE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'YSAINTE ', 'YSTE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' ST ', ' SAINT ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' STE ', ' SAINTE ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' -', ' ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'-', ' ').astype(str)
  df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'  ', ' ').astype(str)
  
  #Filtrage sur les rues
  df_fichier = df_fichier[df_fichier.nature_voie.str.contains('RUE')]
  
  #Remise à zéro de l'index
  df_fichier = df_fichier.reset_index()
  
  #Elimination des colonnes inutiles
  df_fichier =df_fichier.drop(['index', 'DIR', 'CODECOM', 'LIBCOM', 'nature_voie', 'DEP', 'CODEVOIE', 'CLEFVOIE', 'TYPEVOIE', 'Unnamed: 9'], axis=1)
  
  #Elimination des espaces en début/fin de chaîne en passant par une liste
  list_of_single_column = df_fichier['LIBVOIE'].tolist()
  y = 0
  while y < len(list_of_single_column):
      list_of_single_column[y] = list_of_single_column[y].strip()
      y = y+1
  df_fichier = pd.DataFrame(list_of_single_column, columns = ['RUES'])
  returnValue = df_fichier.to_csv('fichier%s.csv' % i, index = False)
  i = i+1



"""**1.3- Concaténer les fichiers dans un dataframe**"""

"""
Opération : concaténer les fichiers dans un dataframe
Output : 1 fichier csv et 1 dataframe de 786 000 entrées
"""

#Création d'un unique fichier csv

path = "/content/"
file_list = [path + f for f in os.listdir(path) if f.startswith('fichier')]
csv_list = []

for file in sorted(file_list):
    csv_list.append(pd.read_csv(file, sep=';', encoding='latin-1', error_bad_lines=False).assign(File_Name = os.path.basename(file)))

csv_merged = pd.concat(csv_list, ignore_index=True)
csv_merged.to_csv(path + 'fichiertotal.csv', index=False)

#Création du dataframe

df = pd.read_csv('/content/fichiertotal.csv')
df = df.drop(['File_Name'], axis=1)

"""# 2. Enrichir les données avec WikiData

**2.1- Sélection des noms de rues à plus de 5 occurrences**
"""

"""
Opération : sélection des noms de rues à plus de 5 occurrences
Output : création d'un df et d'une liste avec les 13 600 noms des rues identifiées
"""

#Création du dataframe filtré sur les noms à plus de 5 occurrences
df2 = df['RUES'].value_counts().rename_axis('rues').reset_index(name='nombre')
df_mask=df2['nombre']>=5
df2 = df2[df_mask]
returnValue = df2.to_csv('fichiertotal_occurrences_filtre5.csv', index = False)

#Transformation du dataframe en liste
list_of_single_column_rue = list(df2['rues'])

"""**2.2- Création de listes de données pour chaque entrée humaine**"""

"""
Opération : création de listes de données Wikidata pour chaque entrées humaines
Source : https://www.wikidata.org/
Output : 4 listes qui comprennent des données sur le genre, l'origine, l'occupation et la description des personnalités qui ont au moins cinq rues à leur nom.
La liste "liste_enrichi_type" détermine si le nom de rue correspond à un humain
La liste "liste_enri_wikicode" donne l'identifiant Wikidata de chaque entrée
"""

#Création des listes
liste_enrichissement_wikicode = []
liste_enrichissement_type = []
liste_enrichissement_genre = []
liste_enrichissement_occupation = []
liste_enrichissement_origine = []
liste_enrichissement_description = []

#Lancement de la requête Wikidata pour chaque entrée
url = "https://www.wikidata.org/w/api.php"
i = 0
while i < len(list_of_single_column_rue):

  query = list_of_single_column_rue[i]
  params = {
      "action" : "wbsearchentities",
      "language" : "en", 
      "format" : "json", 
      "search" : query,
      "limit" : "1"
  }

  data = requests.get(url, params=params)
  wikicondition = data.json()

  if len(wikicondition) == 4:

    #Récupération des l'ID Wikidata
    wikicode = data.json()["search"][0]["id"]

    page = f"https://www.wikidata.org/wiki/{wikicode}"
    r = requests.get(page)
    soup = BeautifulSoup(r.content, "html.parser")

    #Vérification de l'occurrence humaine
    instance = 'human'
    text_instance = soup.find_all('a', text = instance)
    if not text_instance:
      type = "None"
    else:
      type = "Humain"

    facteur_genre = 'sex or gender'
    facteur_occupation = 'occupation'
    facteur_origine = 'country of citizenship'

    text_facteur_genre = soup.find('a', text = facteur_genre)
    text_facteur_occupation = soup.find('a', text = facteur_occupation)
    text_facteur_origine = soup.find('a', text = facteur_origine)

    if not text_facteur_genre or not text_facteur_occupation or not text_facteur_origine:
      genre = "None"
      occupation = "None"
      origine = "None"
      description = "None"
    
    else:
      #Définition des caractéristiques humaines
      if type == "Humain":
  
        #Définition du genre
        gender = 'female'
        text_gender = soup.find_all('a', text = gender) 
        if not text_gender:
          genre = "Homme"
        else:
          genre = "Femme"

        #Définition de l'occupation
        occupation = soup.find('div', {'data-property-id': 'P106'}, low_memory=False).get_text()
        occupation = re.search(r'\n\noccupation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(.*?)\n', occupation).group(1)

        #Définition de l'origine
        origine = soup.find('div', {'data-property-id': 'P27'}, low_memory=False).get_text()
        origine = re.search(r'\n\ncountry of citizenship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(.*?)\n', origine).group(1)

        #Définition de la description
        description = soup.find('span', {'class': 'wikibase-descriptionview-text'}, low_memory=False).get_text()

      else :
        genre = "None"
        occupation = "None"
        origine = "None"
        description = "None"

  else:

    wikicode = 0
    type = "None"
    genre = "None"
    occupation = "None"
    origine = "None"
    description = "None"

  liste_enrichissement_wikicode.append(wikicode)
  liste_enrichissement_type.append(type)
  liste_enrichissement_genre.append(genre)
  liste_enrichissement_occupation.append(occupation)
  liste_enrichissement_origine.append(origine)
  liste_enrichissement_description.append(description)

  i = i+1

"""**2.3- Concaténation des données WikiData avec les noms de rue**"""

"""
Objectif : concaténation des données Wikidata avec les noms de rues
Output : un fichier csv et un df de 13 600 noms de rues, enrichi des données Wikidata pour les personnalités qui figurent dans la liste 
"""

#Transformation des listes WikiData en dataframe
df_wikicode = pd.DataFrame(liste_enrichissement_wikicode, columns = ['wikicode'])
df_type = pd.DataFrame(liste_enrichissement_type, columns = ['type'])
df_genre = pd.DataFrame(liste_enrichissement_genre, columns = ['genre'])
df_occupation = pd.DataFrame(liste_enrichissement_occupation, columns = ['occupation'])
df_origine = pd.DataFrame(liste_enrichissement_origine, columns = ['origine'])
df_description = pd.DataFrame(liste_enrichissement_description, columns = ['description'])

#Fusion des dataframes et enregistrement de la donnée dans un fichier CSV
df_enrichi = pd.concat([df2, df_wikicode, df_type, df_genre, df_occupation, df_origine, df_description], axis=1, join='inner')
returnValue = df_enrichi.to_csv('fichiertotal_occurrences_filtre5_wiki.csv', index = False)

"""**2.4- Filtrage et dédoublonnage des entrées humaines**"""

"""
Objectif : filtrage et dédoublonnage codé des entrées humaines
Outpout : un fichier csv et un df de 1 600 noms de rues, centré sur les personnalités avec les caractéristiques propres à chaque individu
"""

#Transformation du dataframe et liste classée par ordre alphabétique de l'ID Wikidata
df_personnalites = pd.read_csv('/content/fichiertotal_occurrences_filtre5_wiki.csv')
df_personnalites = df_personnalites[['rues','nombre', 'wikicode', 'type', 'genre', 'occupation','origine','description']]
df_personnalites = df_personnalites[(df_personnalites.type != 'None') & (df_personnalites.description != 'None')]
df_personnalites = df_personnalites.sort_values(by=['wikicode'], ascending=True)

#Transformation de chaque colonne du dataframe en listes
list_of_single_column_rue_personnalites = list(df_personnalites['rues'])
list_of_single_column_nombre_personnalites = list(df_personnalites['nombre'])
list_of_single_column_wiki_personnalites = list(df_personnalites['wikicode'])
list_of_single_column_genre_personnalites = list(df_personnalites['genre'])
list_of_single_column_occupation_personnalites = list(df_personnalites['occupation'])
list_of_single_column_origine_personnalites = list(df_personnalites['origine'])
list_of_single_column_description_personnalites = list(df_personnalites['description'])

#Lancement du filtrage pour fusionner les doubles et les triples
y = 0
while y <3:
  i = 0
  while i < (len(list_of_single_column_rue_personnalites) - 1):
    if list_of_single_column_wiki_personnalites[i] == list_of_single_column_wiki_personnalites[i+1]:
      list_of_single_column_nombre_personnalites[i] = list_of_single_column_nombre_personnalites[i] + list_of_single_column_nombre_personnalites[i+1]
      
      list_of_single_column_rue_personnalites.pop(i+1)
      list_of_single_column_nombre_personnalites.pop(i+1)
      list_of_single_column_wiki_personnalites.pop(i+1)
      list_of_single_column_genre_personnalites.pop(i+1)
      list_of_single_column_occupation_personnalites.pop(i+1)
      list_of_single_column_origine_personnalites.pop(i+1)
      list_of_single_column_description_personnalites.pop(i+1)
    i=i+1
  y = y+1

#Transformation des listes en dataframe
df_rues_personnalites = pd.DataFrame(list_of_single_column_rue_personnalites, columns = ['rues'])
df_nombre_personnalites = pd.DataFrame(list_of_single_column_nombre_personnalites, columns = ['nombre'])
df_wiki_personnalites = pd.DataFrame(list_of_single_column_wiki_personnalites, columns = ['wikicode'])
df_genre_personnalites = pd.DataFrame(list_of_single_column_genre_personnalites, columns = ['genre'])
df_occupation_personnalites = pd.DataFrame(list_of_single_column_occupation_personnalites, columns = ['occupation'])
df_origine_personnalites = pd.DataFrame(list_of_single_column_origine_personnalites, columns = ['origine'])
df_description_personnalites = pd.DataFrame(list_of_single_column_description_personnalites, columns = ['description'])

#Fusion des dataframes et enregistrement de la donnée en fichier CSV
df_personnalites_filtre = pd.concat([df_rues_personnalites, df_nombre_personnalites, df_wiki_personnalites, df_genre_personnalites, df_occupation_personnalites, df_origine_personnalites, df_description_personnalites], axis=1, join='inner')
df_personnalites_filtre = df_personnalites_filtre.sort_values(by=['nombre'], ascending=False)
df_personnalites_filtre = df_personnalites_filtre.reset_index()
df_personnalites_filtre = df_personnalites_filtre[['rues','nombre', 'wikicode', 'genre', 'occupation','origine','description']]
returnValue = df_personnalites_filtre.to_csv('fichiertotal_occurrences_filtre5_wiki_filtre_personnalites.csv', index = False)