# -*- coding: utf-8 -*-
"""Street.ipynb

Automatically generated by Colaboratory.


# **Projet : Rues de France**

---
"""

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.patches import ConnectionPatch
import requests
import shutil
import os
import time
import re
from bs4 import BeautifulSoup

"""**1- Intégration des url sources dans une liste**"""

#Création de la première liste d'adresse url de fichiers csv

liste1 = []
page = 1
while page != 10:
      url = f"https://www.lesruesdefrance.com/exportsql/liste_rue_par_dep_0{page}.csv"
      response = requests.get(url)
      liste1.append(url)
      page = page + 1

#Création de la deuxième liste d'adresse url de fichiers csv

liste2 = []
page = 10
while page != 96:
      url = f"https://www.lesruesdefrance.com/exportsql/liste_rue_par_dep_{page}.csv"
      response = requests.get(url)
      liste2.append(url)
      page = page + 1

#Création de la troisième liste d'adresse url de fichiers csv

liste3 = []
page = 971
while page != 975:
      url = f"https://www.lesruesdefrance.com/exportsql/liste_rue_par_dep_{page}.csv"
      response = requests.get(url)
      liste3.append(url)
      page = page + 1

#Création de la quatrième liste d'adresse url de fichiers csv

liste4 = []
page = 976
while page != 977:
      url = f"https://www.lesruesdefrance.com/exportsql/liste_rue_par_dep_{page}.csv"
      response = requests.get(url)
      liste4.append(url)
      page = page + 1

#Fusion des listes

liste = liste1 + liste2 + liste3 + liste4

"""**2- Téléchargement des fichiers sources**"""

#Import des fichiers csv à partir des url présents dans la liste

i = 0
while i != 97:
    r = requests.get(liste[i], stream=True)
    with open("fichier%s.csv" % i, "wb") as f:
        r.raw.decode_content = True
        shutil.copyfileobj(r.raw, f)
    i += 1
    time.sleep(0.5)

"""**3- Nettoyage et filtrage des données**"""

#Création de fichiers uniformisés centrés sur le rues

i = 0
while i <= 96:
  if i == 19:
    i = 20
  else:
    df_fichier = pd.read_csv('/content/fichier%s.csv' % i, sep=';')
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'L\'', 'L ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'D\'', 'D ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'S\'', 'S ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'T\'', 'T ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'  ', ' ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' -', ' ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' TRENTE ', ' 30 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' TRENTE ET UN ', ' 31 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT ET UN ', ' 21 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT DEUX ', ' 22 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT TROIS ', ' 23 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT QUATRE ', ' 24 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT CINQ ', ' 25 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT SIX ', ' 26 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT SEPT ', ' 27 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT HUIT ', ' 28 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT NEUF ', ' 29 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT ', ' 20 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' ONZE ', ' 11 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DOUZE ', ' 12 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' TREIZE ', ' 13 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' QUATORZE ', ' 14 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' QUINZE ', ' 15 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' SEIZE ', ' 16 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DIX SEPT ', ' 17 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DIX HUIT ', ' 18 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DIX NEUF ', ' 19 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' VINGT ', ' 20 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' UN ', ' 1 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DEUX ', ' 2 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' TROIS ', ' 3 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' QUATRE ', ' 4 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' CINQ ', ' 5 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' SIX ', ' 6 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' SEPT ', ' 7 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' HUIT ', ' 8 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' NEUF ', ' 9 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DIX ', ' 10 ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' GEN ', ' GENERAL ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' GAL ', ' GENERAL ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' MAL ', ' MARECHAL ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' COL ', ' COLONEL ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' CDT ', ' COMMANDANT ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' DOC ', ' DOCTEUR ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' GDE ', ' GRANDE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'GEN ', 'GENERAL ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'GAL ', 'GENERAL ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'MAL ', 'MARECHAL ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'COL ', 'COLONEL ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'CDT ', 'COMMANDANT ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'DOC ', 'DOCTEUR ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'GDE ', 'GRANDE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'J J ', 'JEAN JACQUES ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'J P ', 'JEAN PIERRE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'J C ', 'JEAN CLAUDE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'J M ', 'JEAN MARIE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' CHEM ', ' CHEMIN ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'I ET F ', 'IRENE ET FREDERIC ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'P ET M ', 'PIERRE ET MARIE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ST ', 'SAINT ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'STE ', 'SAINTE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ASAINT ', 'AST ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ESAINT ', 'EST ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ISAINT ', 'IST ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'OSAINT ', 'OST ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'USAINT ', 'UST ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'YSAINT ', 'YST ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ASAINTE ', 'ASTE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ESAINTE ', 'ESTE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'ISAINTE ', 'ISTE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'OSAINTE ', 'OSTE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'USAINTE ', 'USTE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'YSAINTE ', 'YSTE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' ST ', ' SAINT ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' STE ', ' SAINTE ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r' -', ' ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'-', ' ').astype(str)
    df_fichier['LIBVOIE'] = df_fichier['LIBVOIE'].str.replace(r'  ', ' ').astype(str)
    #Filtrage sur les rues
    df_fichier = df_fichier[df_fichier.nature_voie.str.contains('RUE')]
    #Remise à zéro de l'index
    df_fichier = df_fichier.reset_index()
    #Elimination des colonnes inutiles
    df_fichier =df_fichier.drop(['index', 'DIR', 'CODECOM', 'LIBCOM', 'nature_voie', 'DEP', 'CODEVOIE', 'CLEFVOIE', 'TYPEVOIE', 'Unnamed: 9'], axis=1)
    #Elimination des espaces en début/fin de chaîne en passant par une liste
    list_of_single_column = df_fichier['LIBVOIE'].tolist()
    y = 0
    while y < len(list_of_single_column):
        list_of_single_column[y] = list_of_single_column[y].strip()
        y = y+1
    df_fichier = pd.DataFrame(list_of_single_column, columns = ['RUES'])
    returnValue = df_fichier.to_csv('fichier%s.csv' % i, index = False)
    i = i+1

"""**3- Fusion des fichiers source**"""

#Fusion des fichiers csv

path = "/content/"
file_list = [path + f for f in os.listdir(path) if f.startswith('fichier')]
csv_list = []

for file in sorted(file_list):
    csv_list.append(pd.read_csv(file, sep=';', encoding='latin-1', error_bad_lines=False).assign(File_Name = os.path.basename(file)))

csv_merged = pd.concat(csv_list, ignore_index=True)
csv_merged.to_csv(path + 'fichiertotal.csv', index=False)

#Import du fichier fusionné dans un dataframe avec un filtre sur les données utiles

df = pd.read_csv('/content/fichiertotal.csv')
df = df.drop(['File_Name', '<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">'], axis=1)

"""**4- Enrichissement des données sur les personnalités avec Wikidata**"""

#Sélection des noms de rues de plus de 5 occurrences
df2 = df['RUES'].value_counts().rename_axis('rues').reset_index(name='nombre')
df_mask=df2['nombre']>=5
df2 = df2[df_mask]
returnValue = df2.to_csv('fichiertotal_occurrences_filtre5.csv', index = False)

#Transformation du dataframe en liste
list_of_single_column_rue = list(df2['rues'])

#Recupération de l'identifiant wikidata pour chaque occurrence humaine
url = "https://www.wikidata.org/w/api.php"

liste_enrichissement_wikicode = []
liste_enrichissement_type = []
liste_enrichissement_genre = []
liste_enrichissement_occupation = []
liste_enrichissement_origine = []
liste_enrichissement_description = []

i = 0
while i < len(list_of_single_column_rue):

  query = list_of_single_column_rue[i]
  params = {
      "action" : "wbsearchentities",
      "language" : "en", 
      "format" : "json", 
      "search" : query,
      "limit" : "1"
  }

  data = requests.get(url, params=params)
  wikicondition = data.json()

  if len(wikicondition) == 4:

    #Récupération des l'ID Wikidata
    wikicode = data.json()["search"][0]["id"]

    page = f"https://www.wikidata.org/wiki/{wikicode}"
    r = requests.get(page)
    soup = BeautifulSoup(r.content, "html.parser")

    #Vérification de l'occurrence humaine
    instance = 'human'
    text_instance = soup.find_all('a', text = instance)
    if not text_instance:
      type = "None"
    else:
      type = "Humain"

    facteur_genre = 'sex or gender'
    facteur_occupation = 'occupation'
    facteur_origine = 'country of citizenship'

    text_facteur_genre = soup.find('a', text = facteur_genre)
    text_facteur_occupation = soup.find('a', text = facteur_occupation)
    text_facteur_origine = soup.find('a', text = facteur_origine)

    if not text_facteur_genre or not text_facteur_occupation or not text_facteur_origine:
      genre = "None"
      occupation = "None"
      origine = "None"
      description = "None"
    
    else:
      #Définition des caractéristiques humaines
      if type == "Humain":
  
        #Définition du genre
        gender = 'female'
        text_gender = soup.find_all('a', text = gender) 
        if not text_gender:
          genre = "Homme"
        else:
          genre = "Femme"

        #Définition de l'occupation
        occupation = soup.find('div', {'data-property-id': 'P106'}, low_memory=False).get_text()
        occupation = re.search(r'\n\noccupation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(.*?)\n', occupation).group(1)

        #Définition de l'origine
        origine = soup.find('div', {'data-property-id': 'P27'}, low_memory=False).get_text()
        origine = re.search(r'\n\ncountry of citizenship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(.*?)\n', origine).group(1)

        #Définition de la description
        description = soup.find('span', {'class': 'wikibase-descriptionview-text'}, low_memory=False).get_text()

      else :
        genre = "None"
        occupation = "None"
        origine = "None"
        description = "None"

  else:

    wikicode = 0
    type = "None"
    genre = "None"
    occupation = "None"
    origine = "None"
    description = "None"

  liste_enrichissement_wikicode.append(wikicode)
  liste_enrichissement_type.append(type)
  liste_enrichissement_genre.append(genre)
  liste_enrichissement_occupation.append(occupation)
  liste_enrichissement_origine.append(origine)
  liste_enrichissement_description.append(description)

  i = i+1

#Transformation des listes en dataframe
df_wikicode = pd.DataFrame(liste_enrichissement_wikicode, columns = ['wikicode'])
df_type = pd.DataFrame(liste_enrichissement_type, columns = ['type'])
df_genre = pd.DataFrame(liste_enrichissement_genre, columns = ['genre'])
df_occupation = pd.DataFrame(liste_enrichissement_occupation, columns = ['occupation'])
df_origine = pd.DataFrame(liste_enrichissement_origine, columns = ['origine'])
df_description = pd.DataFrame(liste_enrichissement_description, columns = ['description'])

#Fusion des dataframes et enregistrement de la donnée en fichier CSV
df_enrichi = pd.concat([df2, df_wikicode, df_type, df_genre, df_occupation, df_origine, df_description], axis=1, join='inner')
returnValue = df_enrichi.to_csv('fichiertotal_occurrences_filtre5_wiki.csv', index = False)

#Cumul des valeurs en double et suppression des doublons
df_personnalites = pd.read_csv('/content/fichiertotal_occurrences_filtre5_wiki.csv')
df_personnalites = df_personnalites.sort_values(by=['wikicode'], ascending=True)

list_of_single_column_rue_personnalites = list(df_personnalites['rues'])
list_of_single_column_nombre_personnalites = list(df_personnalites['nombre'])
list_of_single_column_wiki_personnalites = list(df_personnalites['wikicode'])
list_of_single_column_genre_personnalites = list(df_personnalites['genre'])
list_of_single_column_occupation_personnalites = list(df_personnalites['occupation'])
list_of_single_column_origine_personnalites = list(df_personnalites['origine'])
list_of_single_column_description_personnalites = list(df_personnalites['description'])

y = 0
while y <3:
  i = 0
  while i < (len(list_of_single_column_rue_personnalites) - 1):
    if list_of_single_column_wiki_personnalites[i] == list_of_single_column_wiki_personnalites[i+1]:
      list_of_single_column_nombre_personnalites[i] = list_of_single_column_nombre_personnalites[i] + list_of_single_column_nombre_personnalites[i+1]
      
      list_of_single_column_rue_personnalites.pop(i+1)
      list_of_single_column_nombre_personnalites.pop(i+1)
      list_of_single_column_wiki_personnalites.pop(i+1)
      list_of_single_column_genre_personnalites.pop(i+1)
      list_of_single_column_occupation_personnalites.pop(i+1)
      list_of_single_column_origine_personnalites.pop(i+1)
      list_of_single_column_description_personnalites.pop(i+1)
    i=i+1
  y = y+1

#Transformation des listes en dataframe
df_rues_personnalites = pd.DataFrame(list_of_single_column_rue_personnalites, columns = ['rues'])
df_nombre_personnalites = pd.DataFrame(list_of_single_column_nombre_personnalites, columns = ['nombre'])
df_wiki_personnalites = pd.DataFrame(list_of_single_column_wiki_personnalites, columns = ['wikicode'])
df_genre_personnalites = pd.DataFrame(list_of_single_column_genre_personnalites, columns = ['genre'])
df_occupation_personnalites = pd.DataFrame(list_of_single_column_occupation_personnalites, columns = ['occupation'])
df_origine_personnalites = pd.DataFrame(list_of_single_column_origine_personnalites, columns = ['origine'])
df_description_personnalites = pd.DataFrame(list_of_single_column_description_personnalites, columns = ['description'])

#Fusion des dataframes et enregistrement de la donnée en fichier CSV
df_personnalites_filtre = pd.concat([df_rues_personnalites, df_nombre_personnalites, df_wiki_personnalites, df_genre_personnalites, df_occupation_personnalites, df_origine_personnalites, df_description_personnalites], axis=1, join='inner')
df_personnalites_filtre = df_personnalites_filtre.sort_values(by=['nombre'], ascending=False)
returnValue = df_personnalites_filtre.to_csv('fichiertotal_occurrences_filtre5_wiki_filtre_personnalites.csv', index = False)